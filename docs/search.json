[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "I’m a PhD student in Psychology at Universidad Autónoma de Madrid, where I work on developing and validating psychometric models for experimental data. I also enjoy doing outreach in statistics for psychology.\nThe most unexpected plot twist in my journey is that, after a long time as a frequentist, I’ve finally surrendered to the Bayesian dark side of probability.\n\n    \n    \n  \n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Supplementary Reflections ",
    "section": "",
    "text": "There are questions that don’t fit the word limit, models that evolve after submission, and explanations that only make sense once everything is published. If I tried to include all those details, I would probably end up writing a paper of disclaimers and lose the reader in an endless maze of footnotes.\nInstead, I keep some of those insights here: small reflections and more detailed explanations about specific aspects of my articles that deserve a slower, clearer space. These notes are not extensions of the papers but parallel stories, the parts of research that stay hidden behind results, now given a place to breathe."
  },
  {
    "objectID": "Model diagrams/mod_diagrams.html",
    "href": "Model diagrams/mod_diagrams.html",
    "title": "Bayesian and SEM model diagrams",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "Greetings, Stranger!\n\nPlease have a seat (if you haven’t already), and let me introduce myself. I am a Bayesian psychometrician and expert in Bayesian statistical modeling, working at the intersection of psychometrics and experimental psychology. Currently, I’m doing my PhD at the Cognition, Attention and Learning Lab at Universidad Autónoma de Madrid, Spain.\nMy research aims to bridge psychometrics and experimental psychology by developing and validating psychometric measurement models tailored to experimental paradigms. These models allow researchers to estimate the latent common factors underlying multiple experimental tasks, leading to a more robust and generalizable understanding of cognitive processes."
  },
  {
    "objectID": "about/index.html#my-first-love-psychology",
    "href": "about/index.html#my-first-love-psychology",
    "title": "Ricardo Rey-Sáez",
    "section": "My first love: Psychology",
    "text": "My first love: Psychology\nMy journey in psychology began at the Complutense University of Madrid, where I first aspired to become a clinical psychologist (oh, dear…). That interest naturally drew me toward psychopathology and the basic psychological processes underlying it, such as attention and memory. However, life had other plans: I ended up pursuing a PhD not in therapy or diagnosis but in how to measure basic psychological processes accurately within experimental designs.\nHow did I end up here? Out of skepticism, and to be honest, a bit of ignorance. The more I engaged with research, the more I realized how little I understood about the statistical machinery behind our conclusions. Reading papers without understanding their methods began to feel like an act of faith, and I wasn’t comfortable with that. To overcome it, I took what seemed like a small detour that ultimately redirected my career: I immersed myself in methodology, statistics, and psychometrics to figure out how the models we use actually work, especially when they fail and produce statistical artifacts that researchers might mistakenly treat as substantive evidence. In short, I’m a bit of a skeptic."
  },
  {
    "objectID": "about/index.html#my-true-love-psychometrics",
    "href": "about/index.html#my-true-love-psychometrics",
    "title": "Ricardo Rey-Sáez",
    "section": "My true love: Psychometrics",
    "text": "My true love: Psychometrics\n\n\nTo me, psychometrics is the most important discipline in psychology. It safeguards the quality of our measurements by examining the sources of validity evidence and the reliability of psychological constructs. Without measurement, there is no research; and with measurement, every substantive conclusion ultimately depends on the strength of the validity evidence supporting the interpretation of those scores. How could I not fall in love with that?\nPsychometrics starts from a simple yet powerful idea: what we observe are imperfect reflections of latent constructs that cannot be directly measured. These reflections show up in different indicators, such as questionnaire items, task performances, or behavioral measures, each capturing the construct in its own imperfect way. Psychometric models help us make sense of that imperfection by linking observed variables to latent constructs, accounting for measurement error, and allowing us to study structural relations among cognitive constructs.\nThat perspective changed the way I understood research. Measurement stopped being a technical step and became a way of thinking about how we connect data to theory. In fact, the substantive conclusions researchers care about most are precisely the structural part of psychometric models: the relations among latent constructs that give psychological theory its meaning.\n\n\n\n\n\n\nTypical SEM diagram: squares are observed variables, circles are latent constructs. The example shows a latent mediation model.\n\n\n\n\n\n\n\nTypical SEM diagram: squares are observed variables, circles are latent constructs. The example shows a latent mediation model."
  },
  {
    "objectID": "about/index.html#my-toxic-love-bayesian-statistics",
    "href": "about/index.html#my-toxic-love-bayesian-statistics",
    "title": "Ricardo Rey-Sáez",
    "section": "My toxic love: Bayesian Statistics",
    "text": "My toxic love: Bayesian Statistics\n\n\n\n[]\n\n\n\n\n\nDiagram of a Bayesian hierarchical model applied in our replication study on the unconscious working memory effect.\n\n\n\n\n\nBruno de Finetti once said that “probability does not exist.” It sounds absurd at first, almost like a philosophical trick, but think about something as simple as deciding where to have dinner. You are at home, and you remember the new restaurant that just opened in your neighborhood. It is late and you have no idea if it will still be open, but you have to decide whether to go or not. If you leave, it means you believe there is more than a fifty percent chance it will be open. But the restaurant being open is not probable; it is already a fact! You just do not know it yet. And that is what De Finetti meant. Probability is not a feature of the world; it is not in the restaurant. It lives in what we know and in how we think.\nDe Finetti’s provocative idea planted a new seed of doubt about my own work, since every frequentist model deals with probability in one way or another. What began as a small detour into Bayesian statistics quickly became an obsession, and, unexpectedly, it helped me understand frequentist methods more clearly than ever before. In fact, all the frequentist statistics we know are simply a particular case of Bayesian statistics, much like a golden retriever is just one breed of dog. And to be precise, the frequentist version is probably the most cumbersome of all the Bayesian ones.\nMaybe that is why those who discover Bayesian statistics rarely go back. It is not just a different way of analyzing data, but a more flexible way of thinking. In fact, it allows you to build and test virtually any model you can imagine, to quantify the uncertainty in a natural and intuitive way, and to work without relying on asymptotic assumptions that often make little sense in practice. In my view, the Bayesian approach, especially in applied research, is far more transparent and honest about its assumptions than any frequentist model."
  },
  {
    "objectID": "blog/model_definition/index.html",
    "href": "blog/model_definition/index.html",
    "title": "What Is a Statistical Model? From Density to Likelihood",
    "section": "",
    "text": "Love is probably one of the most familiar words in the world. Ask a room full of people if they know what love is, and almost everyone will say yes. But ask them to define it, and you’ll get as many different answers as people in the room, each focusing on different aspects of what it means to love. In fact, we are all aware of this paradox: everyone believes they know what love is, although each person understands it in their own way. The same thing, on a smaller scale, happens with the word model in the scientific community.\nThere are many “last names” for the word model, such as statistical, theoretical, computational, or predictive, which makes it a bit harder to know exactly what someone means when they use it, especially when they forget to mention which type of model they’re talking about. However, they all share a common idea: a model is a simplified representation of reality, a way to describe how we believe something works.\nThis post is for researchers who already use models and want a clear, friendly look at how they work under the hood. We’ll touch on some technical ideas, like probability density and maximum likelihood, but in a way that feels intuitive and approachable. We will use R to illustrate some of the most important concepts and to recreate the figures, but it is not required to follow this post (at least in the first part). By the end, you may look at your own work differently and you may start asking probabilistic questions about your data. At least, that’s what happened to me."
  },
  {
    "objectID": "blog/model_definition/index.html#the-statistical-model",
    "href": "blog/model_definition/index.html#the-statistical-model",
    "title": "What Is a Statistical Model? From Density to Likelihood",
    "section": "The statistical model",
    "text": "The statistical model\nThe reason we need statistical models is simple: reality changes. Not all students perform equally well in statistics, not all patients get better in the same way, and not every day has the same temperature. Even when conditions look identical, the results change. This variability is everywhere and is, in many ways, what makes the world both interesting and complex. Statistical models exist precisely to deal with that variability.\nA simple example: age and height\nImagine that we measure the height of a group of people and also record their age. In general, we will see that younger children tend to be shorter and that height increases with age. But that relationship is not perfect: two people of the same age can differ by several centimeters. If we wanted to capture that trend with a formula, we could write a simple linear regression model:\n\\[\n\\text{Height}_i = \\beta_0 + \\beta_1 \\cdot \\text{age}_i + \\varepsilon_i\n\\]\nThis small equation expresses a simple idea: height increases with age, but never exactly in the same way. There is always a margin of error, represented by \\(\\varepsilon_i\\), which reflects natural variability between people. But this variability, this \\(\\varepsilon_i\\), is not a minor detail. It is what makes this model statistical rather than deterministic. In statistics, we assume that this unexplained part, this variability, follows a certain pattern. The most common way to describe that pattern is to assume that the errors follow a normal distribution with mean zero and some standard deviation, which we call \\(\\sigma_\\varepsilon\\):\n\\[\n\\varepsilon_{i}\\sim\\mathrm{Normal}\\left(\\mu = 0,\\;\\sigma = \\sigma_\\varepsilon\\right)\n\\]\nThis means that, on average, the model does not make systematic errors (because the mean of the errors is zero) and that the variability of those differences can be summarized by a single parameter, \\(\\sigma_\\varepsilon\\).\nNow I am going to do a magic trick. The two equations we just wrote can be summarized in a single, simpler one that reflects the same idea:\n\\[\n\\text{Height}_i \\sim \\mathrm{Normal}\\left(\\mu =\\beta_0+\\beta_1\\cdot\\text{age}_i,\\;\\sigma = \\sigma_\\varepsilon\\right)\n\\]\nThis notation reads as follows: the height of person \\(i\\) follows a normal distribution whose mean is \\(\\beta_0 + \\beta_1 \\cdot \\text{age}_i\\) and whose standard deviation is \\(\\sigma_\\varepsilon\\). In other words, the model specifies a conditional distribution for height given age. That is, for each value of age, height follows a normal distribution with a different mean but the same dispersion around that mean. Therefore, people of the same age share the same conditional distribution.\nIt may sound abstract, but the figure below makes it much clearer, showing the three conditional distributions of height for ages 5, 10, and 20.\n\n\n\n\n\n\n\n\nWhat the model tells us is which ranges of height have higher or lower probability given a certain age1. For instance, for a 20-year-old, the probability of being somewhere around 175 cm is much greater than the probability of being around 100 cm (though the latter is not zero). This is the kind of information the model provides: a quantitative description of the probability assigned to different ranges of outcomes, given (or not) a set of predictors.\nIn other words, the model speaks the language of probability, as we will see in more detail later on. And this is, in fact, the fundamental idea of a statistical model: we assign a probability distribution to our outcome variable, height. In our example, this distribution is a conditional distribution: for each age value, height has its own distribution, but that is not a requirement. We can also specify models that do not depend on other variables (often called unconditional or marginal distributions), although they are usually less informative."
  },
  {
    "objectID": "blog/model_definition/index.html#the-components-of-a-statistical-model",
    "href": "blog/model_definition/index.html#the-components-of-a-statistical-model",
    "title": "What Is a Statistical Model? From Density to Likelihood",
    "section": "The components of a statistical model",
    "text": "The components of a statistical model\nSince a statistical model is based on assigning a probability distribution to the data, \\(Y\\), it becomes easier to distinguish its components. For example, the regression model above can be written in general form as: \\[\n{\\color{seagreen}Y_i} \\sim {\\color{#7E57C2}\\mathrm{Normal}}\\!\\left(\n  {\\color{#9a2515}\\mu} = {\\color{#0197FD}\\beta_0} + {\\color{#0197FD}\\beta_1}\\cdot{\\color{darkorange}X_i},\\,\n  {\\color{#9a2515}\\sigma} = {\\color{#0197FD}\\sigma_\\varepsilon}\n\\right)\n\\]\nThis notation condenses, at a glance, all the components that make up a statistical model:\n\nA dependent variable and a probability distribution,\nThe parameters of the selected probability distribution,\nThe estimated parameters that sometimes depend on other observable or latent variables\n\n\nOnce you understand the idea of modeling, you can do almost anything. For example, we could model the variance, \\(\\color{#9a2515}{\\sigma}\\), instead of the mean, \\(\\color{#9a2515}{\\mu}\\). That would mean we assume all people have more or less the same height, regardless of age, and that what changes with age is the variability of height.\n\\[\n{\\color{seagreen}Y_i} \\sim {\\color{#7E57C2}\\mathrm{Normal}}\\!\\left(\n  {\\color{#9a2515}\\mu} = {\\color{#0197FD}\\mu_Y},\\,\n  {\\color{#9a2515}\\sigma} = \\exp\\!\\left({\\color{#0197FD}\\beta_0} + {\\color{#0197FD}\\beta_1}\\cdot{\\color{darkorange}X_i}\\right)\n\\right)\n\\]\nThe exponential function ensures that the standard deviation is not negative. Although a model assuming height does not vary with age would be a candidate for the worst model ever, modeling dispersion can be very useful when there is systematic heteroskedasticity, which is not uncommon in applied research.\nChanging the distribution: from Normal to Bernoulli\nMoving away from the normal distribution does not really change the logic; it lets us choose a model that better fits our data. Imagine flipping a coin with your right hand (\\(X_i = 1\\)) or left hand (\\(X_i = 0\\)). The outcome can be heads (\\(Y_i=1\\)) or tails (\\(Y_i=0\\)). For binary outcomes, a normal model is suboptimal: it can predict values outside [0, 1] and doesn’t model probabilities directly. Thus, a better model is\n\\[\n{\\color{seagreen}Y_i} \\sim {\\color{#7E57C2}\\mathrm{Bernoulli}}\\!\\left(\n  {\\color{#9a2515}p} = \\frac{1}{1 + \\exp\\!\\left(-\\big(\n    {\\color{#0197FD}\\beta_0} + {\\color{#0197FD}\\beta_1}\\cdot{\\color{darkorange}X_i}\n  \\big)\\right)}\n\\right)\n\\]\nThis is the well-known logistic regression: modeling a Bernoulli outcome with the logit link. We map the linear predictor, \\(\\beta_0+\\beta_1\\cdot X_i\\), to a valid probability via the logistic function, \\(\\frac{1}{1+\\exp(-x)}\\), which guarantees values are always between 0 and 1.\nModeling counts: Poisson regression\nThe same applies when we have count data, such as the number of times you have thought about quitting this post to do something more productive. Suppose I know whether you studied psychology or not (our new variable \\(X_i\\)). The result is a Poisson regression model:\n\\[\n{\\color{seagreen}Y_i} \\sim {\\color{#7E57C2}\\mathrm{Poisson}}\\!\\left(\n  {\\color{#9a2515}\\lambda} =\n  \\exp\\!\\left(\n    {\\color{#0197FD}\\beta_0} + {\\color{#0197FD}\\beta_1}\\cdot{\\color{darkorange}X_i}\n  \\right)\n\\right)\n\\]\nThat way, I can even tell whether psychologists are the first to run away from this post. It might not make the cover of Nature, but it proves that even procrastination can be elegantly captured by a model.\nChoose your own distribution\nAlthough these three regressions are “classical” because they are included in commercial software like SPSS, the modeling idea extends to any probability distribution. For example, if I am analyzing response times in an experimental task, I can use another distribution that fits their asymmetric shape better, such as the ex-Gaussian distribution. This distribution represents the convolution of a normal and an exponential distribution, and its parameters are \\(\\mu\\), \\(\\sigma\\), and \\(\\tau\\):\n\\[\n{\\color{seagreen}Y_i} \\sim {\\color{#7E57C2}\\text{ex-Gaussian}}\\!\\left(\n  {\\color{#9a2515}\\mu} = {\\color{#0197FD}\\beta_0} + {\\color{#0197FD}\\beta_1}\\cdot{\\color{darkorange}X_i},\\,\n  {\\color{#9a2515}\\sigma} = {\\color{#0197FD}\\sigma},\\,\n  {\\color{#9a2515}\\tau} = {\\color{#0197FD}\\tau}\n\\right)\n\\]\nAs we’ve seen so far, the logic is always the same: you can switch models as easily as you change shoes. The more models you know, the more options you’ll have to find the one that best fits your data."
  },
  {
    "objectID": "blog/model_definition/index.html#the-common-language-of-statistical-models",
    "href": "blog/model_definition/index.html#the-common-language-of-statistical-models",
    "title": "What Is a Statistical Model? From Density to Likelihood",
    "section": "The common language of statistical models",
    "text": "The common language of statistical models\nBy now, we have seen that statistical models can take many forms. Some describe heights, others probabilities, counts, or reaction times. They can use normal, Bernoulli, Poisson, or ex-Gaussian distributions. No matter how different they look, they all speak the same language: probability.\nEvery model tells us how probability is spread across possible outcomes. With discrete data, the model assigns a probability to each outcome, which is fairly intuitive. With continuous data, individual points have probability zero, so the model specifies a probability density, describing how probability over ranges of values is allocated. Still, the message remains the same: for the same variable and units, higher probability values (with discrete data) or higher density values (with continuous data) correspond to outcomes to which the model assigns greater support, given its parameters and covariates. And this allows us to achieve two of the most fundamental goals in statistics: identify, within each model, the parameter values that maximize the joint likelihood of the whole dataset, and compare models by asking under which model the observed data have higher joint likelihood.\nFor example, imagine I have 100 observed values from an exponential distribution with rate \\(\\lambda=2\\):\n\n# Simulate exponential distribution values\nset.seed(123)\nY &lt;- rexp(n = 100, rate = 2)\n\nWe now want to see how three different models assign density values to the same data point under fixed reference parameters. For simplicity, we’ll focus on the first observation of our variable as the reference value. The models considered are a Normal (\\(\\mu=1.5\\), \\(\\sigma=1\\)), a \\(\\chi^2\\) (\\(\\nu=2\\)), and an exponential (\\(\\lambda=2\\)). The Normal and \\(\\chi^2\\) parameter values were chosen arbitrarily, simply to illustrate the idea; the exponential one matches the parameter value used to generate the data.\nIn R, dnorm(), dchisq(), and dexp() are density functions that return the value of the probability density at a given point given specific parameter values. The first argument is the point (e.g., Y[1]), and the remaining arguments set the parameters of the distribution (e.g., mean and sd for dnorm, df for dchisq, rate for dexp).\n\n# Compute density values at the same observed point\ndata.frame(\n  normal = dnorm(Y[1], mean = 1.5, sd = 1),\n  chi_sq = dchisq(Y[1], df = 2),\n  exp    = dexp(Y[1], rate = 2)\n)\n\n     normal   chi_sq       exp\n1 0.2230692 0.404942 0.8604411\n\n\nFor this particular observation, the exponential model assigns the highest density (as you’d expect, since it matches the data-generating process), though this won’t necessarily happen for every observation. To make this more intuitive, the plot below displays the three density curves together, with a dotted vertical line showing our observed value. The height of each curve at that line corresponds exactly to the density values we computed with dnorm(), dchisq() and dexp().\n\n\n\n\n\n\n\n\nThese three density curves are, in fact, the models themselves. For our dependent variable (here, y, which takes only one observed value in this simple illustration) we have assigned a a probability distribution (in this case, three different ones), each depending on its own parameters (which we deliberately fixed) and covariates (none in this example).\nWhat matters here isn’t “who wins”, but that each model speaks the same language at the very same data point: they all return a density that tells us how plausible that value is under each model. Naturally, this depends on the parameter values. We have set \\(\\lambda = 2\\) for the exponential model, matching the data-generating rate; within the same model, different parameter values will yield different densities at the same observed point.\n\n# Density values at the same observed point for different rates\ndata.frame(\n  exp_l1 = dexp(Y[1], rate = 1),\n  exp_l2 = dexp(Y[1], rate = 2),\n  exp_l3 = dexp(Y[1], rate = 3),\n  exp_l4 = dexp(Y[1], rate = 4),\n  exp_l5 = dexp(Y[1], rate = 5)\n)\n\n    exp_l1    exp_l2    exp_l3    exp_l4    exp_l5\n1 0.655912 0.8604411 0.8465605 0.7403589 0.6070129\n\n\nThe plot below illustrates how the exponential density curve changes as we vary the rate parameter \\(\\lambda\\). The dotted vertical line marks the observed value, and the height of each curve at that point corresponds to the density values we just computed. As \\(\\lambda\\) increases, the model assigns less probability mass to larger \\(y\\) values and higher density near zero.\n\n\n\n\n\n\n\n\nWe could keep trying numbers, right? In fact, we could pick the value of \\(\\lambda\\) that gives the highest density value for this first observed response. To see this more clearly, we will vary the rate parameter over a dense grid and check, for the first observation, which value gives the largest density. The code below builds the grid, evaluates the exponential density at that data point for each rate, and returns the rate with the highest value.\n\n# Grid with 9000 rate values\nlambda_vals &lt;- seq(1, 10, .001)\ndensity_vals &lt;- dexp(Y[1], lambda_vals)\nlambda_vals[which.max(density_vals)]\n\n[1] 2.371\n\n\nAnd this value is slightly above the true value of \\(\\lambda = 2\\).\nEverything we’ve done so far is just for illustration. In practice, no one would compare models or estimate parameters using a single observed value. What we really care about is how the model behaves across the entire dataset. Still, we’ll deliberately stay with this one-point example for a bit longer, since it provides a simple way to introduce the concept of likelihood, which is formally defined over the complete dataset.\nProbability, density or likelihood?\nConfusion between likelihood, probability, and density is common when one first ventures into statistical modeling. Building on our previous example, we’ll join the infinite number of attempts, in all their versions and languages, to clarify what each of these terms really means.\nThe difference between probability and density we have already seen:\n\nWith discrete data, models assign a probability value to each possible value.\nWith continuous data, in contrast, models assign a density value to each point, and the probability is obtained by considering ranges of values.\n\nHowever, the difference between probability or density and likelihood is not in the number we calculate, but in what we treat as the free variable.\nIn our first example, we fixed all parameter values to arbitrary numbers (\\(\\mu=1.5\\), \\(\\sigma=1\\), \\(\\nu=2\\), \\(\\lambda=2\\)), acting as omnipotent gods, and calculated the density of one observed value. In statistical jargon, this means treating the parameters as fixed values and treating the observed value as a realization of a random variable. In doing so, we answer the question:\n\nWhat is the probability (or density) of this observed value, given these parameters?\n\nHere the focus is on the data under a specific parameter value.\nIn our second example, we did the opposite: we kept the observed value constant and tested many values of the parameter \\(\\lambda\\) to see how the density changed. In statistical jargon, this means treating the parameter as the variable of interest and treating the data as fixed values. Now the focus is on the parameter, and we ask:\n\nHow compatible is each parameter value with the observed data?\n\nFor a fixed observation, \\(y\\), higher density at that point under a given parameter value means higher likelihood, that is, greater compatibility with the data. In fact, we estimated the parameter value that maximized the density; the only difference is that here, instead of calling it density, we say it maximized the likelihood, even though the number is exactly the same2.\nIn case any reader feels unsure at this point, here’s an even clearer illustration:\n\ndexp(x = 1, rate = 1)\n\n[1] 0.3678794\n\n\nFor an exponential model, the density evaluated at \\(x=1\\) given \\(\\lambda = 1\\) is 0.3678794. At the same time, the likelihood of \\(\\lambda\\) evaluated at \\(\\lambda = 1\\) given \\(x=1\\) is the same number: 0.3678794. Same value, different role. You can even tell which side of the story we’re on just by the “evaluated at”: it quietly reveals whether we’re treating the data or the parameter as the thing that varies.\nMaximum Likelihood\nTrying different parameter values until finding the one that maximizes the likelihood is the basic idea of Maximum Likelihood Estimation. The key difference is that likelihood is calculated using all the data at once, rather than a single observation. Still, the logic is exactly the same as what we have done so far: it all builds on how the model assigns a density to each individual observation.\nThe joint likelihood of \\(n\\) independent and identically distributed observations is the product of the probabilities/densities of each observation:\n\\[\n\\mathcal{L}\\left({\\color{#9a2515}\\theta}\\mid {\\color{seagreen}Y}\\right) = \\prod^n_{i=1} {\\color{#7E57C2}f}\\left({\\color{seagreen}Y_i}\\mid{\\color{#9a2515}\\theta}\\right).\n\\]\nHere, \\({\\color{seagreen}Y}\\) represents all the observed data, \\({\\color{#7E57C2}f}\\left({\\color{seagreen}Y_i}\\mid{\\color{#9a2515}\\theta}\\right)\\) is the probability density (or probability mass) that the model assigns to the value \\({\\color{seagreen}Y_i}\\), and \\({\\color{#9a2515}\\theta}\\) stands for the parameters of the model. For example, the mean and standard deviation in a normal model. For our exponential example:\n\\[\n\\mathcal{L}\\left({\\color{#9a2515}\\lambda}\\mid{\\color{seagreen}Y}\\right)\n= \\prod^n_{i=1}\n{\\color{#7E57C2}\\mathrm{Exponential}}\\!\\left(\n  {\\color{seagreen}Y_i}\\mid{\\color{#9a2515}\\lambda}\n\\right)\n\\]\nDon’t be intimidated by the strange and slightly dramatic look of the equation. Its apparent complexity contrasts with the absolute simplicity of how the very same idea is written in R: it’s just the product of the densities of all observations. Assuming \\(\\lambda = 2\\):\n\n# Likelihood\nprod(dexp(Y, rate = 2))\n\n[1] 4.875431e-16\n\n\nWhy a product? Because we assume that all observations are independent. Under this assumption, the joint probability (or joint density) of observing them all together is the product of the individual probabilities (or densities). This rule follows directly from Kolmogorov’s axioms together with the definition of independence. However, this product can become extremely small as the sample size grows: multiplying many numbers smaller than one can make the result so close to zero that the computer rounds it to zero, a problem known as numerical underflow.\nTo avoid this, we work with the logarithm of the likelihood, which turns the product into a sum and keeps the values within a more stable numerical range. Since the logarithm is a monotonic transformation, maximizing the log-likelihood is equivalent to maximizing the likelihood: \\[\n\\log \\mathcal{L}\\left({\\color{#9a2515}\\theta}\\mid{\\color{seagreen}Y}\\right)\n= \\sum^n_{i=1}\n\\log\\,{\\color{#7E57C2}f}\\!\\left(\n  {\\color{seagreen}Y_i}\\mid{\\color{#9a2515}\\theta}\n\\right)\n\\]\nFor our exponential example, this becomes \\[\n\\log \\mathcal{L}\\left({\\color{#9a2515}\\lambda}\\mid{\\color{seagreen}Y}\\right)\n= \\sum^n_{i=1}\n\\log\\!\\left(\n  {\\color{#7E57C2}\\mathrm{Exponential}}\\!\\left(\n    {\\color{seagreen}Y_i}\\mid{\\color{#9a2515}\\lambda}\n  \\right)\n\\right)\n\\]\nAs before, what looks heavy in notation turns out to be delightfully simple in R. The joint log-likelihood is just the sum of log-densities. For the exponential model evaluated at \\(\\lambda = 2\\):\n\n# Log-Likelihood\nsum(dexp(Y, rate = 2, log = TRUE))\n\n[1] -35.25715\n\n\nWe can also illustrate how to compute the joint log-likelihood under different models evaluated at the same reference parameters:\n\n# Compute Log-Likelihoods assuming fixed parameter values\ndata.frame(\n  normal = sum(dnorm(Y, mean = 1.5, sd = 1, log = TRUE)),\n  chi_sq = sum(dchisq(Y, df = 2, log = TRUE)),\n  exp    = sum(dexp(Y, rate = 2, log = TRUE))\n)\n\n     normal    chi_sq       exp\n1 -152.9833 -95.45769 -35.25715\n\n\nHere, higher (less negative) values indicate better model fit for these fixed parameter choices.\nThis is a simple illustration but not a fair comparison between the three models, because we have not estimated the parameters using the same data. In fact, we have not estimated anything by maximum likelihood yet. The next code shows how to obtain \\(\\lambda\\) for the exponential model and \\(\\nu\\)3 for the \\(\\chi^2\\) model by computing the log-likelihood with all the data at once and choosing the value that maximizes it:\n\n# Grids with 49000 rate and nu values\nlambda_vals &lt;- nu_vals &lt;- seq(1, 50, .001)\n\n# Vector to store log-likelihood values\nll_exp_vals &lt;- ll_chisq_vals &lt;- numeric(length(lambda_vals))\n\n# For loop: compute log-likelihood for each lambda value\nfor (i in seq_along(lambda_vals)) {\n  ll_exp_vals[i] &lt;- sum(dexp(Y, rate = lambda_vals[i], log = TRUE))\n  ll_chisq_vals[i] &lt;- sum(dchisq(Y, df = nu_vals[i], log = TRUE))\n}\n\n# Maximum-likelihood lambda estimate\nMLE_lambda &lt;- lambda_vals[which.max(ll_exp_vals)]\nMLE_nu     &lt;- nu_vals[which.max(ll_chisq_vals)]\n\nFor the normal distribution, we need to estimate two parameters: \\(\\mu\\) and \\(\\sigma\\). To keep the illustration consistent with the other two models, we’ll estimate them “by brute force”: we create a matrix with two columns, one for \\(\\mu\\) values and one for \\(\\sigma\\) values. Each row represents a combination of \\(\\mu\\) and \\(\\sigma\\) where we evaluate the log-likelihood, and we then select the row that maximizes it.\n\n# Mu and sigma values\nmu_vals &lt;- seq(min(Y), max(Y), .01)\nsigma_vals &lt;- seq(.0001, sd(Y) * 3, .01)\n\n# All combinations of mu and sigma values\ncomb_vals &lt;- expand.grid(mu = mu_vals, sigma = sigma_vals)\n\n# Empty vector to save log-likelihood values\nll_vals &lt;- numeric(nrow(comb_vals))\n\n# Save values for each combination of mu and sigma\nfor(i in 1:nrow(comb_vals)){\n  ll_vals[i] &lt;- sum(dnorm(Y, mean = comb_vals[i,1], sd = comb_vals[i,2], log = TRUE))\n}\n\n# Maximum-likelihood estimators\nMLE_mu &lt;- comb_vals[which.max(ll_vals),\"mu\"]\nMLE_sigma &lt;- comb_vals[which.max(ll_vals),\"sigma\"]\n\nWith the maximum likelihood estimates obtained, we can compute the total log-likelihood for all data under each model using the estimated parameters. The following code returns a table with these three log-likelihoods.\n\n# Compute Log-Likelihoods using ML estimates\ndata.frame(\n  normal = sum(dnorm(Y, mean = MLE_mu, sd = MLE_sigma, log = TRUE)),\n  chi_sq = sum(dchisq(Y, df = MLE_nu, log = TRUE)),\n  exp    = sum(dexp(Y, rate = MLE_lambda, log = TRUE))\n)\n\n     normal    chi_sq       exp\n1 -75.87007 -59.26746 -35.15573\n\n\nWhat we are doing here is informative, but in real applications it would not be enough to decide which model fits best. More robust approaches build precisely on the same idea (the log density as a measure of fit) but extend it through cross-validation or information criteria that penalize model complexity, such as AIC or BIC. The goal here is simply to show how likelihood provides the foundation for these more advanced methods."
  },
  {
    "objectID": "blog/model_definition/index.html#posterior-reflections4",
    "href": "blog/model_definition/index.html#posterior-reflections4",
    "title": "What Is a Statistical Model? From Density to Likelihood",
    "section": "Posterior Reflections4\n",
    "text": "Posterior Reflections4\n\nNow that we’ve gone through it all, we can finally say what a statistical model is, both in spirit and in form. Formally, a model is a collection of probability distributions, one for each value of its parameters5. Each distribution tells us how probability is spread across the data we might see, and it can be evaluated at the data we actually observed. When we hold the data fixed and let the parameters vary, the same object gives us the likelihood, which measures how compatible each parameter value is with what we saw.\nThat’s the common language behind every model, from the simplest linear regression to the most elaborate hierarchical model. Once you see that, statistics stops looking like a forest of formulas and starts feeling like what it truly is: a way of reasoning about uncertainty.\nWhen you understand this, the rest of statistics starts to unfold naturally. If a model can assign probability to what we’ve seen, it can help us find the parameter values that make those observations most likely (something we’ve already done throughout this post) and also quantify how uncertain we are about those estimates (a step we haven’t explored here). From there, we can evaluate whether one explanation fits the data better than another, and even use what we’ve learned to predict what future data might look like. It’s the same reasoning, just applied in different directions.\nSo if someone asks whether you know what a statistical model is, you can smile and say: “It’s probable, in every sense of the word 6.”"
  },
  {
    "objectID": "blog/model_definition/index.html#extra-beyond-the-grid",
    "href": "blog/model_definition/index.html#extra-beyond-the-grid",
    "title": "What Is a Statistical Model? From Density to Likelihood",
    "section": "Extra: beyond the grid",
    "text": "Extra: beyond the grid\nMaximum likelihood with optimization algorithms\nThe grid approach we used is useful for teaching but inefficient in practice. As the number of parameters grows, it quickly becomes computationally impossible. For this reason, maximum likelihood estimation is usually obtained using numerical optimization algorithms.\nWe will not go into the details of these methods here, but it is useful to see how they work in practice. The code below shows how to use the optim function in R to estimate the parameters by maximum likelihood for the three models. The results are equivalent to those from the grid search, but this approach can generalize to a much wider range of situations.\n\n# Log-likelihood functions: gaussian model\nll_norm &lt;- function(par, y){\n  mu        &lt;- par[1]\n  sigma     &lt;- exp(par[2])              # sigma &gt; 0\n  sum(dnorm(y, mean = mu, sd = sigma, log = TRUE))\n}\n\n# Log-likelihood functions: exponential model\nll_exp &lt;- function(par, y){\n  lambda &lt;- exp(par[1])                 # lambda &gt; 0\n  sum(dexp(y, rate = lambda, log = TRUE))\n}\n\n# Log-likelihood functions: chi-square model\nll_chisq &lt;- function(par, y){\n  nu &lt;- exp(par[1])                     # nu &gt; 0\n  sum(dchisq(y, df = nu, log = TRUE))\n}\n\n# Starting values for the optimizer\nnorm_svals &lt;- c(.1,.1)\nexp_svals &lt;- .1 \nchisq.svals &lt;- .1\n\n# ML estimation via optim\nfit_norm  &lt;- optim(norm_svals,  ll_norm,  y = Y, method = \"BFGS\", \n                   control = list(fnscale = -1))\nfit_exp   &lt;- optim(exp_svals, ll_exp,   y = Y, method = \"BFGS\", \n                   control = list(fnscale = -1))\nfit_chisq &lt;- optim(chisq.svals, ll_chisq, y = Y, method = \"BFGS\", \n                   control = list(fnscale = -1))\n\n# Extract MLEs\nMLE_mu     &lt;- fit_norm$par[1]\nMLE_sigma  &lt;- exp(fit_norm$par[2])\nMLE_lambda &lt;- exp(fit_exp$par[1])\nMLE_nu     &lt;- exp(fit_chisq$par[1])\n\n# Log-likelihoods at MLEs\nLL_normal &lt;- fit_norm$value\nLL_exp    &lt;- fit_exp$value\nLL_chisq  &lt;- fit_chisq$value\n\n# Summary table\nMLE_table &lt;- data.frame(\n  model = c(\"Normal\", \"Exponential\", \"Chi-square\"),\n  param_1 = c(sprintf(\"mu = %.6f\", MLE_mu),\n              sprintf(\"lambda = %.6f\", MLE_lambda),\n              sprintf(\"nu = %.6f\", MLE_nu)),\n  param_2 = c(sprintf(\"sigma = %.6f\", MLE_sigma), \"\", \"\"),\n  logLik  = c(LL_normal, LL_exp, LL_chisq),\n  row.names = NULL\n)\n\nMLE_table\n\n        model           param_1          param_2    logLik\n1      Normal     mu = 0.522859 sigma = 0.516706 -75.86575\n2 Exponential lambda = 1.912560                  -35.15572\n3  Chi-square     nu = 1.040636                  -59.26745\n\n\nClosed-form maximum likelihood solutions\nIf you made it this far, congratulations: we have gone step by step through the full path of maximum likelihood estimation. We have tried values by hand, done grid searches, and used a numerical optimizer. And now, just when it seems we have done it all, comes a small plot twist: in some cases, everything can be solved with a single formula.\nIt almost sounds unfair, right? After spending so much effort understanding the logic of likelihood, it turns out that for some distributions the exact solution can be written in one line. But that is precisely why the long path is worth it: those formulas are no longer mysterious recipes, but the natural conclusion of a story that now makes sense.\nFor the Normal and Exponential models, the maximum likelihood estimators have closed-form solutions, meaning they can be obtained directly from the properties of the distributions without iterative algorithms.\nFor the Normal distribution, the parameters that maximize the likelihood are the sample mean and the sample standard deviation (using \\(1/n\\) instead of \\(1/(n-1)\\) because here we want the value that maximizes the likelihood, not an unbiased estimator):\n\\[\n\\hat\\mu_\\text{ML}=\\frac{1}{n}\\cdot\\sum^n_{i=1}Y_{i},\\qquad \\hat\\sigma_\\text{ML}=\\sqrt{\\frac{1}{n}\\sum^n_{i=1}\\left(Y_i-\\hat\\mu_\\text{ML}\\right)^2}\n\\]\n\n(MLE_mu_closed    &lt;- mean(Y))\n\n[1] 0.5228594\n\n(MLE_sigma_closed &lt;- sqrt(mean((Y - MLE_mu_closed)^2)))\n\n[1] 0.5167061\n\n\nFor the Exponential distribution, the maximum likelihood estimator of the rate parameter \\(\\lambda\\) is also obtained directly:\n\\[\n\\hat\\lambda_\\text{ML}=\\frac{n}{\\sum^n_{i=1}Y_i}\n\\]\n\n(MLE_lambda_closed &lt;- length(Y) / sum(Y))\n\n[1] 1.91256\n\n\nThese two expressions exactly match the values we obtained with the optimization algorithm. For the \\(\\chi^2\\) distribution, however, there is no closed-form solution, and its estimation necessarily requires numerical methods."
  },
  {
    "objectID": "blog/model_definition/index.html#footnotes",
    "href": "blog/model_definition/index.html#footnotes",
    "title": "What Is a Statistical Model? From Density to Likelihood",
    "section": "Footnotes",
    "text": "Footnotes\n\nNote that we are not talking about the probability of a single, exact value. For continuous variables, that probability is always zero. What continuous models provide instead is a density, which tells us how probability is distributed across possible values.↩︎\nAny outraged reader may address their complaints to Sir Ronald Fisher, or demand that, before naming things, statisticians must submit each new term to a linguistic ethics committee.↩︎\nWe’ll let \\(\\nu\\) be any positive real number. Although textbooks often introduce \\(\\nu\\) as an integer (“degrees of freedom”), the \\(\\chi^2\\) family extends smoothly to non-integers and is equivalent to a Gamma distribution with shape = \\(\\nu/2\\) and scale = 2. We do this just to keep the example simple and focus on likelihood. (And yes, in R, dchisq() works for non-integer \\(\\nu\\)).↩︎\nYou came here knowing something, you’ve read the post, and now what you knew might have shifted a bit, huh? I know, I know… I’ll see myself out… and wait for you there…↩︎\nEven more formally, a (parametric) statistical model is a family of distributions \\(\\mathcal{M}=\\left\\{{\\color{#7E57C2}f}\\left({\\color{seagreen}y}\\mid{\\color{#9a2515}\\theta}\\right):{\\color{#9a2515}\\theta}\\in{\\color{#9a2515}\\Theta}\\right\\}\\), where \\({\\color{#7E57C2}f}\\left({\\color{seagreen}y}\\mid{\\color{#9a2515}\\theta}\\right)\\) is a probability density (or mass) function on the sample space (the set of all possible values that the data \\({\\color{seagreen}y}\\) can take), and \\({\\color{#9a2515}\\Theta}\\) is the parameter space (the set of all possible values that the parameters \\({\\color{#9a2515}\\theta}\\) can take).↩︎\nHowever, be careful when the same person asks you what probability is. That’s how most of us die.↩︎"
  },
  {
    "objectID": "Model diagrams/mod_diagrams_dark.html",
    "href": "Model diagrams/mod_diagrams_dark.html",
    "title": "Bayesian and SEM model diagrams",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "CV/index.html",
    "href": "CV/index.html",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "2024–present | Ph.D. Psychology, Universidad Autónoma de Madrid\n\nDeveloping and validating psychometric models for the study of individual differences in experimental psychology.\n\n2020–2022 | M.S. Quantitative Research Methods for Behavioral and Health Sciences, Complutense University of Madrid\n2017–2020 | B.S. Psychology, Complutense University of Madrid\n\n\n\n\n\n2024 | Bayesian Cognitive Modeling: A Practical Course, University of Amsterdam\n2023 | Practical Introduction to Advances in Exploratory Factor Analysis Applied to Item Analysis, University of the Basque Country\n2022 | Introduction to Exploratory Graph Analysis (EGA) with R, Zaragoza University\n2020 | Principles, Statistical and Computational Tools for Reproducible Data Science, Harvard University (HarvardX) | Verified Certificate\n2020 | Professional Certificate in Data Science, Harvard University\n\nData Science: R basics | Verified Certificate\n\nData Science: Visualization | Verified Certificate\n\nData Science: Probability | Verified Certificate\n\nData Science: Inference and Modeling | Verified Certificate\n\nData Science: Productivity Tools | Verified Certificate\n\nData Science: Wrangling | Verified Certificate\n\nData Science: Linear Regression | Verified Certificate\n\n\n\n\n\n\n2025 | Introduction to Bayesian Psychometrics: Applications to Factor Analysis\n\nSeminar instructor, National interfaculty training seminar\n\nOrganized by ASCOFAPSI (Colombian Association of Faculties of Psychology)\n\n2025 | Beyond Maximum Likelihood: Introduction to Bayesian Factor Analysis\n\nSeminar instructor, Master’s program in Quantitative Research Methods\n\nOrganized by Autonomous University of Madrid\n\n2024 | ChatGPT and Psychometrics: Optimizing the Design, Refinement, and Data Analysis of Measurement Instruments\n\nSeminar instructor, Master’s program in Quantitative Research Methods\n\nOrganized by Autonomous University of Madrid\n\n2023–2024 | Data Analysis in Psychology\n\nCourse instructor, Undergraduate program (Bachelor’s in Psychology)\n\nOrganized by Nebrija University\n\n2023 | Adapting Our Teaching to the Revolution of AI and ChatGPT\n\nWorkshop instructor, Faculty training\n\nOrganized by Universidad Autónoma de Madrid\n\n2023 | How can we Adapt Our Teaching to the Revolution of AI and ChatGPT?\n\nWorkshop instructor, Faculty training\n\nOrganized by University of Costa Rica\n\n\n\n\n\n\nRey-Sáez, R., & Estrada, E. (2023). Análisis de datos con R en Ciencias Sociales y de la Salud. Editorial Síntesis: https://www.sintesis.com/libro/analisis-de-datos-con-r-en-ciencias-sociales-y-de-la-salud\n\n\n\n\n\n2025 | Best Oral Communication Award | Ibero-American Association for Research on Individual Differences\n\nPresentation Title: New Hierarchical Models to Answer Old Psychometric Questions in Experimental Research: Which Task Best Measures Inhibition?\n\nPrize: MASK-5 Measurement Instrument\n\n2024 | Dissemination and Transfer in Methodology Award | Spanish Association of Methodology for Behavioral Sciences\n\nProject Ecuación Triunfo\n\nFunding: €1,000\n\n2024 | Julio Olea Grant | Spanish Association of Methodology for Behavioral Sciences\n\nFunding to carry out the research project The union of psychometrics and experimental psychology: Bayesian hierarchical models with common factors for response times\n\nFunding: €2,400\n\n\n\n\n\nFranco-Martínez, A., Rey-Sáez, R., Shanks, D. R., Soto, D., & Vadillo, M. A. (in press). Replicating the unconscious working memory effect: A multisite preregistered study. Neuroscience of Consciousness. https://doi.org/10.1093/nc/niaf046\n\nData Code PDF OSF\n\nPacheco-Romero, A. M., Martín-García, Ó., Rey-Sáez, R., Boemo, T., Blanco, I., Vázquez, C., & Sánchez-López, Á. (2024). An integrative analysis of potential mechanisms of reduced positive affect in daily life in depression: an ESM study. Cognition and Emotion, 38(4), 587–604. PDF\n\nData Code PDF OSF\n\nGracia, E., Martín-Fernández, M., Rey-Sáez, R., & Yount, K. M. (2024). Attitudes justifying partner violence against women in Latin-American and Caribbean countries: A measurement invariance study. Psychology of Violence, 14(2), 127–136. https://doi.org/10.1037/vio0000497\n\nData Code PDF OSF\n\nVega-Sanz, M., Rey-Sáez, R., Berástegui, A., & Sánchez-López, Á. (2024). Difficulties in pregnancy adjustment as predictors of perinatal depression: Indirect effects through the role of brooding rumination and maternal–fetal bonding. American Journal of Perinatology, 41(S 01), e2870–e2877. https://doi.org/10.1055/s-0043-1776062\n\nData Code PDF OSF\n\nFranco-Martínez, A., Rey-Sáez, R., & Castillejo, I. (2023). The seven wonderings of large language models as psychometric designers, refiners, and analysts. PsyArXiv. https://doi.org/10.31234/osf.io/kmqy5\n\nData Code PDF OSF\n\nRey-Sáez, R. (2022). Réplica a: Validación cruzada sobre una misma muestra: Una práctica sin fundamento. R.E.M.A. Revista Electrónica de Metodología Aplicada, 24(1), 41–44. PDF\n\nData Code PDF OSF\n\n\n\n\n\nRey-Sáez, R., Franco-Martínez, A., Revuelta, J., & Vadillo, M. A. (2025). New Hierarchical Models to Answer Old Psychometric Questions in Experimental Research: Which Task Best Measures Inhibition? [Oral presentation] XIV Conference – Asociación Iberoamericana para la Investigación de las Diferencias Individuales (AIIDI).\nRey-Sáez, R., Franco-Martínez, A., Revuelta, J., & Vadillo, M. A. (2025). Where psychometrics meets experimental psychology: Bayesian hierarchical factor models for response times. [Oral presentation] XI Conference – European Congress of Methodology.\nRey-Sáez, R., Revuelta, J., & Vadillo, M. A. (2025). From random effects to common factors: Latent dimensionality assessment in experimental psychology. [Oral presentation] XI Conference – European Congress.\nRey-Sáez, R., Garre-Frutos, F., Franco-Martínez, A., Castillejo, I., & Vadillo, M. A. (2025). Addressing methodological challenges in unconscious process research: A hierarchical modeling approach. [Poster presentation] 27 Annual Meeting of the Association for Scientific Studies of Consciousness.\nRey-Sáez, R., Garre-Frutos, F., Franco-Martínez, A., Castillejo, I., & Vadillo, M. A. (2025). Hierarchical models in experimental psychology: A solution for measuring unconscious psychological processes. [Poster presentation] XIV Reunión Científica sobre Atención.\nRey-Sáez, R., Garre-Frutos, F., & Vadillo, M. Ángel. (2024). What psychometrics can offer to experimental psychology: The multilevel model as the legacy of factor analysis. [Oral presentation] XVIII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R., Lecuona, Ó., & Martín-Fernández, M. (2024). Approximate measurement invariance with multiple groups: A comparison between Bayesian SEM and the alignment method. [Oral presentation] XVIII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nLecuona, Ó., Rey-Sáez, R., & Martín-Fernández, Manuel. (2024). Studying invariance with multiple groups in large datasets: A comparison of bayesian SEM and alignment method. [Oral presentation] 13th Conference of The International Test Commission.\nRey-Sáez, R., Franco-Martínez, A., Castillejo, I., & Vadillo, M. A. (2024). From unconscious to unreliable? Measurement error biases evidence on studies of unconscious processing. [Poster presentation] 27 Annual Meeting of the Association for Scientific Studies of Consciousness.\nFranco-Martínez, A., Rey-Sáez, R., Shanks, D. R., Soto, D., & Vadillo, M. A. (2024). Advances in “replicating the unconscious working memory effect: A multisite preregistered study.” [Poster presentation] 27 Annual Meeting of the Association for Scientific Studies of Consciousness.\nPacheco-Romero, A. M., Martín-Gracía, Ó., Rey-Sáez, R., Boemo, T., Blanco, I., & Sánchez-López, Á. (2023). Affective reactivity to daily events predicts changes in future psychopathology and is modulated by the use of different forms of rumination. [Oral presentation] International Convention of Psychological Science (ICPS).\nAivar, P., Rey-Sáez, R., & Vadillo, M. A. (2023). Hard to ignore? Irrelevant distractors cannot be completely suppressed in a contextual cueing task. [Poster presentation] 23 Annual Meeting of the Vision Sciences Society.\nMartin-Garcia, O., Rey-Sáez, R., Boemo, T., Pacheco-Romero, A., Blanco, I., & Sanchez-Lopez, A. (2023). Modulatory role of motivational self-focuses on the relation between perceived stress and momentary affect: An ESM study. [Oral presentation] Emotions Congress 2023.\nPacheco-Romero, A. M., Martín-Gracía, Ó., Rey-Sáez, R.., Boemo, T., Blanco, I., Vázquez, C., & Sánchez-López, Á. (2022). Intra-individual mechanisms of positive affect regulation deficits in depression: An experience sampling study. [Oral presentation] 52nd European Association for Behavioural; Cognitive Therapies.\nRey-Sáez, R., & Nájera, P. (2022). Investigando metodología en el multiverso de la academia: Comunidad, recursos y estrategias por y para jóvenes. [Symposium Organizer] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R. (2022). Investigación, paso 1: ¿Cómo y dónde busco información? Nuevas herramientas para la detección eficiente de bibliografía de interés. [Oral presentation] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R. (2022). Approximate measurement invariance: Assessing the accuracy and sensitivity of alignment optimization under partial invariance with multiple groups. [Oral presentation] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nMartín-Fernández, M., & Rey-Sáez, R. (2022). Approximate measurement invariance in surveys on attitudes toward intimate partner violence in 12 Latin American countries. [Poster presentation] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R. (2022). Approximate measurement invariance in surveys on attitudes toward intimate partner violence in 12 Latin American countries. [Oral presentation] Research Conference of the Faculty of Psychology, Universidad Autónoma de Madrid"
  },
  {
    "objectID": "CV/index.html#education",
    "href": "CV/index.html#education",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "2024–present | Ph.D. Psychology, Universidad Autónoma de Madrid\n\nDeveloping and validating psychometric models for the study of individual differences in experimental psychology.\n\n2020–2022 | M.S. Quantitative Research Methods for Behavioral and Health Sciences, Complutense University of Madrid\n2017–2020 | B.S. Psychology, Complutense University of Madrid"
  },
  {
    "objectID": "CV/index.html#additional-training",
    "href": "CV/index.html#additional-training",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "2024 | Bayesian Cognitive Modeling: A Practical Course, University of Amsterdam\n2023 | Practical Introduction to Advances in Exploratory Factor Analysis Applied to Item Analysis, University of the Basque Country\n2022 | Introduction to Exploratory Graph Analysis (EGA) with R, Zaragoza University\n2020 | Principles, Statistical and Computational Tools for Reproducible Data Science, Harvard University (HarvardX) | Verified Certificate\n2020 | Professional Certificate in Data Science, Harvard University\n\nData Science: R basics | Verified Certificate\n\nData Science: Visualization | Verified Certificate\n\nData Science: Probability | Verified Certificate\n\nData Science: Inference and Modeling | Verified Certificate\n\nData Science: Productivity Tools | Verified Certificate\n\nData Science: Wrangling | Verified Certificate\n\nData Science: Linear Regression | Verified Certificate"
  },
  {
    "objectID": "CV/index.html#teaching-and-educational-activities",
    "href": "CV/index.html#teaching-and-educational-activities",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "2025 | Introduction to Bayesian Psychometrics: Applications to Factor Analysis\n\nSeminar instructor, National interfaculty training seminar\n\nOrganized by ASCOFAPSI (Colombian Association of Faculties of Psychology)\n\n2025 | Beyond Maximum Likelihood: Introduction to Bayesian Factor Analysis\n\nSeminar instructor, Master’s program in Quantitative Research Methods\n\nOrganized by Autonomous University of Madrid\n\n2024 | ChatGPT and Psychometrics: Optimizing the Design, Refinement, and Data Analysis of Measurement Instruments\n\nSeminar instructor, Master’s program in Quantitative Research Methods\n\nOrganized by Autonomous University of Madrid\n\n2023–2024 | Data Analysis in Psychology\n\nCourse instructor, Undergraduate program (Bachelor’s in Psychology)\n\nOrganized by Nebrija University\n\n2023 | Adapting Our Teaching to the Revolution of AI and ChatGPT\n\nWorkshop instructor, Faculty training\n\nOrganized by Universidad Autónoma de Madrid\n\n2023 | How can we Adapt Our Teaching to the Revolution of AI and ChatGPT?\n\nWorkshop instructor, Faculty training\n\nOrganized by University of Costa Rica"
  },
  {
    "objectID": "CV/index.html#textbooks",
    "href": "CV/index.html#textbooks",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "Rey-Sáez, R., & Estrada, E. (2023). Análisis de datos con R en Ciencias Sociales y de la Salud. Editorial Síntesis: https://www.sintesis.com/libro/analisis-de-datos-con-r-en-ciencias-sociales-y-de-la-salud"
  },
  {
    "objectID": "CV/index.html#awards",
    "href": "CV/index.html#awards",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "2025 | Best Oral Communication Award | Ibero-American Association for Research on Individual Differences\n\nPresentation Title: New Hierarchical Models to Answer Old Psychometric Questions in Experimental Research: Which Task Best Measures Inhibition?\n\nPrize: MASK-5 Measurement Instrument\n\n2024 | Dissemination and Transfer in Methodology Award | Spanish Association of Methodology for Behavioral Sciences\n\nProject Ecuación Triunfo\n\nFunding: €1,000\n\n2024 | Julio Olea Grant | Spanish Association of Methodology for Behavioral Sciences\n\nFunding to carry out the research project The union of psychometrics and experimental psychology: Bayesian hierarchical models with common factors for response times\n\nFunding: €2,400"
  },
  {
    "objectID": "CV/index.html#publications",
    "href": "CV/index.html#publications",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "Franco-Martínez, A., Rey-Sáez, R., Shanks, D. R., Soto, D., & Vadillo, M. A. (in press). Replicating the unconscious working memory effect: A multisite preregistered study. Neuroscience of Consciousness. https://doi.org/10.1093/nc/niaf046\n\nData Code PDF OSF\n\nPacheco-Romero, A. M., Martín-García, Ó., Rey-Sáez, R., Boemo, T., Blanco, I., Vázquez, C., & Sánchez-López, Á. (2024). An integrative analysis of potential mechanisms of reduced positive affect in daily life in depression: an ESM study. Cognition and Emotion, 38(4), 587–604. PDF\n\nData Code PDF OSF\n\nGracia, E., Martín-Fernández, M., Rey-Sáez, R., & Yount, K. M. (2024). Attitudes justifying partner violence against women in Latin-American and Caribbean countries: A measurement invariance study. Psychology of Violence, 14(2), 127–136. https://doi.org/10.1037/vio0000497\n\nData Code PDF OSF\n\nVega-Sanz, M., Rey-Sáez, R., Berástegui, A., & Sánchez-López, Á. (2024). Difficulties in pregnancy adjustment as predictors of perinatal depression: Indirect effects through the role of brooding rumination and maternal–fetal bonding. American Journal of Perinatology, 41(S 01), e2870–e2877. https://doi.org/10.1055/s-0043-1776062\n\nData Code PDF OSF\n\nFranco-Martínez, A., Rey-Sáez, R., & Castillejo, I. (2023). The seven wonderings of large language models as psychometric designers, refiners, and analysts. PsyArXiv. https://doi.org/10.31234/osf.io/kmqy5\n\nData Code PDF OSF\n\nRey-Sáez, R. (2022). Réplica a: Validación cruzada sobre una misma muestra: Una práctica sin fundamento. R.E.M.A. Revista Electrónica de Metodología Aplicada, 24(1), 41–44. PDF\n\nData Code PDF OSF"
  },
  {
    "objectID": "CV/index.html#conference-presentations",
    "href": "CV/index.html#conference-presentations",
    "title": "Ricardo Rey-Sáez",
    "section": "",
    "text": "Rey-Sáez, R., Franco-Martínez, A., Revuelta, J., & Vadillo, M. A. (2025). New Hierarchical Models to Answer Old Psychometric Questions in Experimental Research: Which Task Best Measures Inhibition? [Oral presentation] XIV Conference – Asociación Iberoamericana para la Investigación de las Diferencias Individuales (AIIDI).\nRey-Sáez, R., Franco-Martínez, A., Revuelta, J., & Vadillo, M. A. (2025). Where psychometrics meets experimental psychology: Bayesian hierarchical factor models for response times. [Oral presentation] XI Conference – European Congress of Methodology.\nRey-Sáez, R., Revuelta, J., & Vadillo, M. A. (2025). From random effects to common factors: Latent dimensionality assessment in experimental psychology. [Oral presentation] XI Conference – European Congress.\nRey-Sáez, R., Garre-Frutos, F., Franco-Martínez, A., Castillejo, I., & Vadillo, M. A. (2025). Addressing methodological challenges in unconscious process research: A hierarchical modeling approach. [Poster presentation] 27 Annual Meeting of the Association for Scientific Studies of Consciousness.\nRey-Sáez, R., Garre-Frutos, F., Franco-Martínez, A., Castillejo, I., & Vadillo, M. A. (2025). Hierarchical models in experimental psychology: A solution for measuring unconscious psychological processes. [Poster presentation] XIV Reunión Científica sobre Atención.\nRey-Sáez, R., Garre-Frutos, F., & Vadillo, M. Ángel. (2024). What psychometrics can offer to experimental psychology: The multilevel model as the legacy of factor analysis. [Oral presentation] XVIII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R., Lecuona, Ó., & Martín-Fernández, M. (2024). Approximate measurement invariance with multiple groups: A comparison between Bayesian SEM and the alignment method. [Oral presentation] XVIII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nLecuona, Ó., Rey-Sáez, R., & Martín-Fernández, Manuel. (2024). Studying invariance with multiple groups in large datasets: A comparison of bayesian SEM and alignment method. [Oral presentation] 13th Conference of The International Test Commission.\nRey-Sáez, R., Franco-Martínez, A., Castillejo, I., & Vadillo, M. A. (2024). From unconscious to unreliable? Measurement error biases evidence on studies of unconscious processing. [Poster presentation] 27 Annual Meeting of the Association for Scientific Studies of Consciousness.\nFranco-Martínez, A., Rey-Sáez, R., Shanks, D. R., Soto, D., & Vadillo, M. A. (2024). Advances in “replicating the unconscious working memory effect: A multisite preregistered study.” [Poster presentation] 27 Annual Meeting of the Association for Scientific Studies of Consciousness.\nPacheco-Romero, A. M., Martín-Gracía, Ó., Rey-Sáez, R., Boemo, T., Blanco, I., & Sánchez-López, Á. (2023). Affective reactivity to daily events predicts changes in future psychopathology and is modulated by the use of different forms of rumination. [Oral presentation] International Convention of Psychological Science (ICPS).\nAivar, P., Rey-Sáez, R., & Vadillo, M. A. (2023). Hard to ignore? Irrelevant distractors cannot be completely suppressed in a contextual cueing task. [Poster presentation] 23 Annual Meeting of the Vision Sciences Society.\nMartin-Garcia, O., Rey-Sáez, R., Boemo, T., Pacheco-Romero, A., Blanco, I., & Sanchez-Lopez, A. (2023). Modulatory role of motivational self-focuses on the relation between perceived stress and momentary affect: An ESM study. [Oral presentation] Emotions Congress 2023.\nPacheco-Romero, A. M., Martín-Gracía, Ó., Rey-Sáez, R.., Boemo, T., Blanco, I., Vázquez, C., & Sánchez-López, Á. (2022). Intra-individual mechanisms of positive affect regulation deficits in depression: An experience sampling study. [Oral presentation] 52nd European Association for Behavioural; Cognitive Therapies.\nRey-Sáez, R., & Nájera, P. (2022). Investigando metodología en el multiverso de la academia: Comunidad, recursos y estrategias por y para jóvenes. [Symposium Organizer] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R. (2022). Investigación, paso 1: ¿Cómo y dónde busco información? Nuevas herramientas para la detección eficiente de bibliografía de interés. [Oral presentation] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R. (2022). Approximate measurement invariance: Assessing the accuracy and sensitivity of alignment optimization under partial invariance with multiple groups. [Oral presentation] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nMartín-Fernández, M., & Rey-Sáez, R. (2022). Approximate measurement invariance in surveys on attitudes toward intimate partner violence in 12 Latin American countries. [Poster presentation] XVII Congreso de Metodología de las Ciencias Sociales y de la Salud.\nRey-Sáez, R. (2022). Approximate measurement invariance in surveys on attitudes toward intimate partner violence in 12 Latin American countries. [Oral presentation] Research Conference of the Faculty of Psychology, Universidad Autónoma de Madrid"
  }
]